import streamlit as st
from streamlit_option_menu import option_menu

# Esto debe ir primero
st.set_page_config(
    page_title="Inicio - PredicciÃ³n de DÃ­gitos",
    initial_sidebar_state="collapsed",
    layout="wide" 
)






st.title("Modelo SVM (Support Vector Machine)")


st.markdown("""
Las **MÃ¡quinas de Vectores de Soporte (SVM)** son modelos de aprendizaje supervisado utilizados principalmente para clasificaciÃ³n, aunque tambiÃ©n pueden aplicarse a regresiÃ³n.

---
""")

st.subheader("ğŸ§  Idea Principal")
st.markdown("""
El objetivo de SVM es **encontrar el mejor lÃ­mite (frontera)** que separe dos clases, maximizando el espacio (margen) entre ellas.
""")

st.image("https://upload.wikimedia.org/wikipedia/commons/7/72/SVM_margin.png", 
caption="El hiperplano separa las dos clases con el mayor margen posible.", width=400)

st.subheader("ğŸ” Componentes Clave")
st.markdown("""
- **Hiperplano**: Es la frontera de decisiÃ³n que separa las clases.
- **Vectores de soporte**: Son los puntos mÃ¡s cercanos al hiperplano. Solo estos puntos afectan directamente a la posiciÃ³n del hiperplano.
- **Margen**: Es la distancia entre el hiperplano y los vectores de soporte. SVM maximiza este margen.

""")

st.subheader("âš™ï¸ Ventajas del modelo SVM")
st.markdown("""
- âœ… Funciona bien en espacios de alta dimensiÃ³n.
- âœ… Eficaz cuando hay una clara separaciÃ³n entre clases.
- âœ… Usa pocos puntos de datos (vectores de soporte) â†’ eficiente.
""")

st.subheader("ğŸ§  Â¿CÃ³mo se aplica SVM a imÃ¡genes?")
st.markdown("""
En problemas como la clasificaciÃ³n de dÃ­gitos (por ejemplo, el dataset de dÃ­gitos de Scikit-learn):

- Cada imagen de 8x8 pÃ­xeles se convierte en un vector de 64 caracterÃ­sticas.
- SVM trata de encontrar un hiperplano en ese espacio que separe dÃ­gitos diferentes (por ejemplo, 3s de 5s).
- Para mÃºltiples clases (0 a 9), SVM entrena varios clasificadores binarios (uno contra uno o uno contra todos).



Vector de 64 caracterÃ­sticas:  
[[ 0.  0.  5. 13.  9.  1.  0.  0.]
[ 0.  0. 13. 15. 10. 15.  5.  0.]
[ 0.  3. 15.  2.  0. 11.  8.  0.]

[ 0.  4. 12.  0.  0.  8.  8.  0.]
[ 0.  5.  8.  0.  0.  9.  8.  0.]
[ 0.  4. 11.  0.  1. 12.  7.  0.]

[ 0.  2. 14.  5. 10. 12.  0.  0.]
[ 0.  0.  6. 13. 10.  0.  0.  0.]]

Cada nÃºmero representa la intensidad del pÃ­xel, pero el rango estÃ¡ entre 0 y 16.

""")

st.subheader("ğŸ¯ Â¿CÃ³mo clasifica SVM varios dÃ­gitos?")

st.markdown("""
SVM, por defecto, **solo sabe distinguir entre dos clases** (por ejemplo, "Â¿es un 3 o no lo es?").

Pero cuando queremos clasificar **dÃ­gitos del 0 al 9**, tenemos **10 clases distintas**.


Para ello, entrena varios clasificadores.  
SVM entrena **varios modelos pequeÃ±os** que comparan solo **dos nÃºmeros a la vez**. Esto se llama:

---

###### 1ï¸âƒ£ Uno contra todos (OvR)
- Crea un modelo para cada nÃºmero.
- Cada modelo aprende: **"Â¿Es un 3 o no lo es?"**, **"Â¿Es un 5 o no lo es?"**, etc.
- Se hacen 10 modelos (uno por cada dÃ­gito).
- El modelo que estÃ© mÃ¡s seguro es el que da la predicciÃ³n final.

###### 2ï¸âƒ£ Uno contra uno (OvO)
- Crea un modelo por **cada par posible de nÃºmeros**:  
  "Â¿Es un 2 o un 3?", "Â¿Es un 7 o un 9?", etc.
- En total se crean **45 modelos** para los 10 dÃ­gitos.
- Todos los modelos votan, y el nÃºmero con mÃ¡s votos gana.


###### âš™ï¸ Scikit-learn (la librerÃ­a que estamos usando) **usa por defecto la estrategia "uno contra uno"**.


""")

st.header("ğŸ“ Ejemplo prÃ¡ctico")
st.code("""
from sklearn.datasets import load_digits
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Cargar dataset
digits = load_digits()
X = digits.data
y = digits.target

# Escalado
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# DivisiÃ³n de datos
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)

# Entrenamiento del modelo SVM
clf = SVC(kernel="linear")
clf.fit(X_train, y_train)
""", language="python")

st.subheader("ğŸ“Š Tipos de kernel")
st.markdown("""
SVM puede usar diferentes funciones para separar los datos:

- `linear`: LÃ­nea o plano recto.
- `poly`: PolinÃ³mico (curvas).
- `rbf`: Radial Basis Function (muy comÃºn para separaciones complejas).
- `sigmoid`: Similar a una red neuronal.

""")



